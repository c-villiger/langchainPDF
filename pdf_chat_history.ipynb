{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tabulate numpy pandas langchain openai chromadb pypdf tiktoken faiss-cpu Flask unstructured Cython pdfminer.six termcolor tabulate tqdm reportlab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loaders\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "# Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "# Embeddings and models\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# Chains\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "# Utils\n",
    "import os\n",
    "from termcolor import colored\n",
    "import textwrap\n",
    "from prettytable import PrettyTable\n",
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Path Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\cesar\\OneDrive\\Desktop\\test2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```qa_single_file```\n",
    "This iterates over each file seperately, asking each one the same question. Good for literature overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_single_file(folder_path, chain_type, chunk_size, query, k, own_knowledge = False, show_pages=False):\n",
    "\n",
    "    # Define output table.\n",
    "    tables = {}\n",
    "\n",
    "    # Wraptext function for prettytable\n",
    "    def wrap_text(text, width=40):\n",
    "        return \"\\n\".join(textwrap.wrap(text, width=width))\n",
    "\n",
    "    \"\"\"\n",
    "    Read-in and split the documents\n",
    "    \"\"\"\n",
    "    # Loop over all files in folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Clear all_pages\n",
    "        all_pages = []\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if file_name.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_name.endswith('.csv'):\n",
    "            loader = CSVLoader(file_path)\n",
    "        elif file_name.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif file_name.endswith('.md'):\n",
    "            loader = UnstructuredMarkdownLoader(file_path)\n",
    "        else:\n",
    "            continue  # Skip files with other extensions\n",
    "\n",
    "        file = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_size/2)\n",
    "        pages = text_splitter.split_documents(file)\n",
    "        all_pages.extend(pages)\n",
    "\n",
    "        if show_pages:\n",
    "            print(len(all_pages))\n",
    "            for page in all_pages:\n",
    "                print(page)\n",
    "\n",
    "        \"\"\"\n",
    "        Vectorstores\n",
    "        \"\"\"\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        db = FAISS.from_documents(all_pages, embeddings)\n",
    "        # FAISS vectorstores can also be merged and saved to disk\n",
    "\n",
    "        \"\"\"\n",
    "        Retriever\n",
    "        \"\"\"\n",
    "        # Amount of returned documents k\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "\n",
    "        \"\"\"\n",
    "        Chains\n",
    "        \"\"\"\n",
    "\n",
    "        # Define Chain\n",
    "        if own_knowledge:\n",
    "            prompt_template = \"\"\"Use the following pieces of context to find an answer to all the keys given in the question. \\\n",
    "                Give your answer in the form of a dictionary with the keys given in the question. \\\n",
    "                If the answer does not become clear from the context, you can also use your own knowledge. \\\n",
    "                If you use your own knowledge, please indicate this clearly in your answer. \\\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            {question}\n",
    "            Helpful answer:\"\"\"\n",
    "\n",
    "        if not own_knowledge:\n",
    "\n",
    "            prompt_template = \"\"\"Use the following pieces of context to find an answer to all the keys given in the question. \\\n",
    "                Give your answer in the form of a dictionary with the keys given in the question. \\\n",
    "                Do NOT use your own knowledge and give the best possible answer from the context.\\\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            {question}\n",
    "            Helpful answer:\"\"\"\n",
    "\n",
    "\n",
    "        PROMPT = PromptTemplate(\n",
    "            template=prompt_template, input_variables=[\"context\",\"question\"]\n",
    "        )\n",
    "\n",
    "        chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "        # Define Chain\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=OpenAI(temperature=0),\n",
    "            chain_type=chain_type,\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs=chain_type_kwargs\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Run Chain with parameters\n",
    "        result = qa(query)\n",
    "\n",
    "        # Convert string representation of dictionary to an actual dictionary\n",
    "        result_dict = ast.literal_eval(result['result'])\n",
    "\n",
    "        # Get Sources\n",
    "        sources = [(os.path.basename(doc.metadata[\"source\"]), f\"page: {doc.metadata['page']}\") for doc in result['source_documents']]\n",
    "\n",
    "        # Append result to output tables\n",
    "        table_key = (file_name, tuple(sources))\n",
    "        if table_key not in tables:\n",
    "            table_columns = [\"Filename\", \"Sources\"] + list(result_dict.keys())\n",
    "            tables[table_key] = PrettyTable(table_columns)\n",
    "        table_row = [wrap_text(table_key[0]), wrap_text(', '.join([f'{source[0]} {source[1]}' for source in table_key[1]]))] + [wrap_text(str(value)) for value in result_dict.values()]\n",
    "        tables[table_key].add_row(table_row)\n",
    "\n",
    "    # Return output tables\n",
    "    return tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"model specification, model estimation, model evaluation, model deployment, benchmark models\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results\n",
    "Use smaller chunk sizes to catch more different part in one prompt. Use at least as many chunks (k) as there are keys in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mFilename: file1.pdf\u001b[0m\n",
      "+-----------+----------------------------------------+---------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+-----------------------------------------+\n",
      "|  Filename |                Sources                 |          model specification          |             model estimation             |             model evaluation             |             model deployment             |             benchmark models            |\n",
      "+-----------+----------------------------------------+---------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+-----------------------------------------+\n",
      "| file1.pdf | file1.pdf page: 8, file1.pdf page: 13, |    Support vector regression (SVR),   | The weights that were generated by these | Validation is an important step that is  | Based on the saved weighted values, the  |  They tested their model against other  |\n",
      "|           | file1.pdf page: 2, file1.pdf page: 1,  | multiple linear regression (MLR), CRT | four algorithms were used as the inputs  |   used to check the performance of the   | future predictions for 10 minutes and 30 | models supported by ELM, NN, and FLANN. |\n",
      "|           |           file1.pdf page: 1            |   regression tree, and partial least  |     of the Cuckoo search algorithm.      | system by comparing the actual data with |    minutes are done and the system’s     |   They found that between ELM, NN, and  |\n",
      "|           |                                        | squares (PLS) regression methods were |                                          |  predicted data. Here we have used MSE   |         performance is measured.         |   FLANN, ELM shows the most effective   |\n",
      "|           |                                        |      used to train their dataset.     |                                          |  (Mean Squared Error), RMSE (Root Mean   |                                          |              optimization.              |\n",
      "|           |                                        |                                       |                                          |    Square Error), MAPE (Mean Absolute    |                                          |                                         |\n",
      "|           |                                        |                                       |                                          |  Percentage Error), MAE (Mean Absolute   |                                          |                                         |\n",
      "|           |                                        |                                       |                                          |      Error), ARV (Absolute Relative      |                                          |                                         |\n",
      "|           |                                        |                                       |                                          |         Variance), and Theils U.         |                                          |                                         |\n",
      "+-----------+----------------------------------------+---------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+-----------------------------------------+\n",
      "\n",
      "\u001b[32mFilename: file2.pdf\u001b[0m\n",
      "+-----------+----------------------------------------+---------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+-----------------------------------------+\n",
      "|  Filename |                Sources                 |          model specification          |             model estimation             |             model evaluation             |             model deployment             |             benchmark models            |\n",
      "+-----------+----------------------------------------+---------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+-----------------------------------------+\n",
      "| file2.pdf | file2.pdf page: 8, file2.pdf page: 13, |    Support vector regression (SVR),   | The weights that were generated by these | Validation is an important step that is  | Based on the saved weighted values, the  |  They tested their model against other  |\n",
      "|           | file2.pdf page: 2, file2.pdf page: 1,  | multiple linear regression (MLR), CRT | four algorithms were used as the inputs  |   used to check the performance of the   | future predictions for 10 minutes and 30 | models supported by ELM, NN, and FLANN. |\n",
      "|           |           file2.pdf page: 1            |   regression tree, and partial least  |     of the Cuckoo search algorithm.      | system by comparing the actual data with |    minutes are done and the system’s     |   They found that between ELM, NN, and  |\n",
      "|           |                                        | squares (PLS) regression methods were |                                          |  predicted data. Here we have used MSE   |         performance is measured.         |   FLANN, ELM shows the most effective   |\n",
      "|           |                                        |      used to train their dataset.     |                                          |  (Mean Squared Error), RMSE (Root Mean   |                                          |              optimization.              |\n",
      "|           |                                        |                                       |                                          |    Square Error), MAPE (Mean Absolute    |                                          |                                         |\n",
      "|           |                                        |                                       |                                          |  Percentage Error), MAE (Mean Absolute   |                                          |                                         |\n",
      "|           |                                        |                                       |                                          |      Error), ARV (Absolute Relative      |                                          |                                         |\n",
      "|           |                                        |                                       |                                          |         Variance), and Theils U.         |                                          |                                         |\n",
      "+-----------+----------------------------------------+---------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Output Table\n",
    "output_table = qa_single_file(folder_path=folder_path, \n",
    "            chain_type=\"stuff\",\n",
    "            chunk_size=500, \n",
    "            query=query, \n",
    "            k=5, \n",
    "            own_knowledge = True, \n",
    "            show_pages=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Print Output Table\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m table_key, table_value \u001b[39min\u001b[39;00m output_table\u001b[39m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(colored(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFilename: \u001b[39m\u001b[39m{\u001b[39;00mtable_key[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(table_value)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_table' is not defined"
     ]
    }
   ],
   "source": [
    "# Print Output Table\n",
    "for table_key, table_value in output_table.items():\n",
    "    print(colored(f\"Filename: {table_key[0]}\", \"green\"))\n",
    "    print(table_value)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```qa_single_file_iterated```\n",
    "This iterates over each file **and key** seperately to be even more accurate with the single answers, asking each one the same question. Good for literature overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_single_file_iterated(folder_path, chain_type, chunk_size, queries, k, num_iterations, own_knowledge = False, show_pages=False):\n",
    "\n",
    "    # Define output table.\n",
    "    tables = {}\n",
    "\n",
    "    # Wraptext function for prettytable\n",
    "    def wrap_text(text, width=80):\n",
    "        return textwrap.fill(text, width=width)\n",
    "\n",
    "    \"\"\"\n",
    "    Read-in and split the documents\n",
    "    \"\"\"\n",
    "    # Loop over all files in folder\n",
    "    for file_name in tqdm(os.listdir(folder_path), desc=\"Processing files\",  colour=\"green\", leave=False):\n",
    "        # Define table for filename\n",
    "        tables[file_name] = {}\n",
    "\n",
    "        # Clear all_pages\n",
    "        all_pages = []\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if file_name.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_name.endswith('.csv'):\n",
    "            loader = CSVLoader(file_path)\n",
    "        elif file_name.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif file_name.endswith('.md'):\n",
    "            loader = UnstructuredMarkdownLoader(file_path)\n",
    "        else:\n",
    "            continue  # Skip files with other extensions\n",
    "\n",
    "        file = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_size/2)\n",
    "        pages = text_splitter.split_documents(file)\n",
    "        all_pages.extend(pages)\n",
    "\n",
    "        if show_pages:\n",
    "            print(len(all_pages))\n",
    "            for page in all_pages:\n",
    "                print(page)\n",
    "\n",
    "        \"\"\"\n",
    "        Vectorstores\n",
    "        \"\"\"\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        db = FAISS.from_documents(all_pages, embeddings)\n",
    "        # FAISS vectorstores can also be merged and saved to disk\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Chains\n",
    "        \"\"\"\n",
    "\n",
    "        # Define Chain\n",
    "        if own_knowledge:\n",
    "            prompt_template = \"\"\"Use the following pieces of context to find an answer to the given question. \\\n",
    "                If the answer does not become clear from the context, you can also use your own knowledge. \\\n",
    "                If you use your own knowledge, please indicate this clearly in your answer. \\\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question:\n",
    "            Based on the context, how does the context define and apply: {question}?\n",
    "            Helpful answer:\"\"\"\n",
    "\n",
    "        if not own_knowledge:\n",
    "\n",
    "            prompt_template = \"\"\"Use the following pieces of context to find an answer to the given question. \\\n",
    "                Do NOT use your own knowledge and give the best possible answer from the context.\\\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question:\n",
    "            Based on the context, how does the context define and apply: {question}?\n",
    "            Helpful answer:\"\"\"\n",
    "\n",
    "\n",
    "        PROMPT = PromptTemplate(\n",
    "            template=prompt_template, input_variables=[\"context\",\"question\"]\n",
    "        )\n",
    "\n",
    "        chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "        \"\"\"\n",
    "        Retriever\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define summary chain\n",
    "        text_splitter = CharacterTextSplitter()\n",
    "        qa_condense = load_summarize_chain(llm=OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Application of Chains\n",
    "        \"\"\"\n",
    "\n",
    "        # Iterate over each query\n",
    "        for query in queries:\n",
    "\n",
    "            extended_answers = []\n",
    "            unique_sources = set()\n",
    "\n",
    "            for i in range(num_iterations):\n",
    "\n",
    "                # QA chain that is adaptable\n",
    "                # Amount of returned documents k-i -> makes it adaptable. Otherwise, it would always return k documents and the output would be the same.\n",
    "                retriever = db.as_retriever(\n",
    "                    search_type=\"similarity\", search_kwargs={\"k\": k-i})\n",
    "\n",
    "                # Define retrieval chain\n",
    "                qa = RetrievalQA.from_chain_type(\n",
    "                    llm=OpenAI(temperature=0),\n",
    "                    chain_type=chain_type,\n",
    "                    retriever=retriever,\n",
    "                    return_source_documents=True,\n",
    "                    chain_type_kwargs=chain_type_kwargs\n",
    "                )\n",
    "\n",
    "                # Run Chain with parameters\n",
    "                result = qa(query)\n",
    "\n",
    "                # Get Sources\n",
    "                sources = [(doc.page_content, os.path.basename(doc.metadata[\"source\"]), f\"page: {doc.metadata['page']}\") for doc in\n",
    "                        result['source_documents']]\n",
    "\n",
    "                # Append result to extended_answers\n",
    "                extended_answers.append(result['result'])\n",
    "\n",
    "                # Add sources to the unique_sources set\n",
    "                unique_sources.update(sources)\n",
    "\n",
    "\n",
    "\n",
    "            # Combine extended_answers\n",
    "            combined_result = ' '.join(extended_answers)\n",
    "\n",
    "            # Run the qa function on the combined_result (summary)\n",
    "            texts = text_splitter.split_text(combined_result)\n",
    "            docs = [Document(page_content=t) for t in texts[:3]]\n",
    "            \n",
    "            condensed_result = str(qa_condense.run(docs))\n",
    "\n",
    "            # Combine unique_sources\n",
    "            combined_sources = {tuple([source[1], source[2]]): source[0] for source in unique_sources}\n",
    "\n",
    "\n",
    "            # Store the results in the tables dictionary\n",
    "            tables[file_name][query] = {\n",
    "                \"combined_results\": combined_result,\n",
    "                \"condensed_result\": condensed_result,\n",
    "                \"combined_sources\": combined_sources\n",
    "            }\n",
    "\n",
    "\n",
    "    # Return output tables\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"model specification\", \"model estimation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results\n",
    "Use smaller chunk sizes to catch more different part in one prompt. Use at least as many chunks (k) as there are keys in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               4.64s/it]\r"
     ]
    }
   ],
   "source": [
    "# Generate Output Table\n",
    "output_table = qa_single_file_iterated(folder_path=folder_path, \n",
    "            chain_type=\"stuff\",\n",
    "            chunk_size=500, \n",
    "            queries=queries, \n",
    "            k=8, \n",
    "            num_iterations=2,\n",
    "            own_knowledge = False, \n",
    "            show_pages=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: file1.pdf\n",
      "+------------------+------------------------------------------+------------------------------------------+\n",
      "| Type             | model specification                      | model estimation                         |\n",
      "+==================+==========================================+==========================================+\n",
      "| Combined_results | The context defines model                | Model estimation is the process of       |\n",
      "|                  | specification as the combination of      | using regression techniques to compare   |\n",
      "|                  | regression techniques with the cuckoo    | actual data with predicted data in order |\n",
      "|                  | search algorithm, inspired by the        | to measure the performance of the        |\n",
      "|                  | autoregressive moving average (ARMA)     | system. This is done by calculating the  |\n",
      "|                  | model, to predict the exchange market.   | MSE, RMSE, MAE, and R-squared (R2)       |\n",
      "|                  | The model is validated against 14886     | values.  Model estimation is the process |\n",
      "|                  | samples for the 10-minute model and 3723 | of using regression techniques to train  |\n",
      "|                  | samples for the 30-minute model, and is  | a dataset and generate weights that can  |\n",
      "|                  | trained using the rest of the data.      | be used to predict future values. The    |\n",
      "|                  | The context defines model specification  | context defines model estimation as      |\n",
      "|                  | as the combination of regression         | using multiple linear regression (MLR),  |\n",
      "|                  | techniques with the cuckoo search        | CRT regression tree, and partial least   |\n",
      "|                  | algorithm to predict the exchange        | squares (PLS) regression methods to      |\n",
      "|                  | market. The model is inspired by the     | train a dataset and generate weights     |\n",
      "|                  | autoregressive moving average (ARMA)     | that can be used to predict future       |\n",
      "|                  | model and is tested against other models | values. The context also applies model   |\n",
      "|                  | such as ELM, NN, and FLANN. The          | estimation by using the generated        |\n",
      "|                  | performance of the system is measured    | weights as inputs for the Cuckoo search  |\n",
      "|                  | using MSE, RMSE, MAE, and R-squared (R2) | algorithm.                               |\n",
      "|                  | values.                                  |                                          |\n",
      "+------------------+------------------------------------------+------------------------------------------+\n",
      "| Condensed_result | This paper presents a model              | Model estimation is the process of       |\n",
      "|                  | specification combining regression       | using regression techniques such as MLR, |\n",
      "|                  | techniques with the cuckoo search        | CRT regression tree, and PLS regression  |\n",
      "|                  | algorithm to predict the exchange        | to train a dataset and generate weights  |\n",
      "|                  | market, inspired by the autoregressive   | that can be used to predict future       |\n",
      "|                  | moving average (ARMA) model. The model   | values. The generated weights are then   |\n",
      "|                  | is tested against other models such as   | used as inputs for the Cuckoo search     |\n",
      "|                  | ELM, NN, and FLANN and is validated      | algorithm to measure the performance of  |\n",
      "|                  | against 14886 samples for the 10-minute  | the system by calculating the MSE, RMSE, |\n",
      "|                  | model and 3723 samples for the 30-minute | MAE, and R-squared (R2) values.          |\n",
      "|                  | model. Performance is measured using     |                                          |\n",
      "|                  | MSE, RMSE, MAE, and R-squared (R2)       |                                          |\n",
      "|                  | values.                                  |                                          |\n",
      "+------------------+------------------------------------------+------------------------------------------+\n",
      "| Combined_sources | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 1\u001b[0m                        | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 9\u001b[0m                        |\n",
      "|                  | exchange  market prediction.  A hybrid   | color, and the model predicted  closing  |\n",
      "|                  | model based on the regression            | values are marked  by blue color.  The   |\n",
      "|                  | technique  was developed  by Said, Omar, | y-axis indicates  the unit value of this |\n",
      "|                  | and Aziz who used a combi-  nation of    | pair which in this case is the           |\n",
      "|                  | regression  techniques  with the cuckoo  | normalized  closing price of EUR/USD     |\n",
      "|                  | search algorithm  [17] .  Their model    | currency  pair. The ﬂuctuation  in  the  |\n",
      "|                  | was inspired  by the autoregressive      | curve indicates  the ups and downs of    |\n",
      "|                  | moving  average  (ARMA)   model and they | the closing prices.  The graphs clearly  |\n",
      "|                  | prepared  their dataset with historical  | show how accurate  the predictions  are: |\n",
      "|                  | data of USD/EUR   currency  pair.        | actual  and predicted  values almost     |\n",
      "|                  | Support  vector regression  (SVR),       | overlap  with each other. The MSE, RMSE, |\n",
      "|                  | multiple  linear regres-                 | and MAE scores of our model for EUR/USD  |\n",
      "|                  | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 7\u001b[0m                        | 10-mins  pairs are 0.00001,              |\n",
      "|                  | system architecture  of our proposed     | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 1\u001b[0m                        |\n",
      "|                  | system.   4.1. Data collection   The     | exchange  market prediction.  A hybrid   |\n",
      "|                  | dataset was collected  from Histdata     | model based on the regression            |\n",
      "|                  | [65] website.  Data was col-  lected for | technique  was developed  by Said, Omar, |\n",
      "|                  | four major currency  pairs: EUR/USD      | and Aziz who used a combi-  nation of    |\n",
      "|                  | [66] , GBP/USD  [67] ,  USD/CAD  [68] ,  | regression  techniques  with the cuckoo  |\n",
      "|                  | and USD/CHF  [69] . We have collected    | search algorithm  [17] .  Their model    |\n",
      "|                  | two years of his-  torical time series   | was inspired  by the autoregressive      |\n",
      "|                  | data from 1st January,  2017 to 31st     | moving  average  (ARMA)   model and they |\n",
      "|                  | December,  2018  for our 10 minutes      | prepared  their dataset with historical  |\n",
      "|                  | prediction  model and from 1st January,  | data of USD/EUR   currency  pair.        |\n",
      "|                  | 2019 to 30th  \u001b[31mfile1.pdf\u001b[0m                  | Support  vector regression  (SVR),       |\n",
      "|                  | \u001b[31mpage: 15\u001b[0m and nonparametric               | multiple  linear regres-                 |\n",
      "|                  | self-organising  modelling  approach,    | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 2\u001b[0m M.S.                   |\n",
      "|                  | Expert Syst. Appl. 36 (10)  (2009)       | Islam and E. Hossain Soft Computing      |\n",
      "|                  | 12001–12011,  doi:                       | Letters 3 (2021) 100009  Fig. 1. Simple  |\n",
      "|                  | 10.1016/j.eswa.2009.03.057  .  [4] R.D.  | architecture  of our proposed  pipeline. |\n",
      "|                  | Huang, R.W. Masulis, Fx spreads and      | USDINR.  They tested their model against |\n",
      "|                  | dealer competition  across the 24-hour   | other models supported  by  ELM, NN, and |\n",
      "|                  | trading day, Rev. Financ. Stud. 12 (1)   | FLANN.  They found that between  ELM,    |\n",
      "|                  | (1999) 61–93, doi: 10.1093/rfs/12.1.61   | NN, and FLANN,   ELM shows the most      |\n",
      "|                  | .  [5] S. Masry, A. Dupuis, R. Olsen, E. | eﬀective  optimization.  Consistent      |\n",
      "|                  | Tsang, Time zone normalization  of fx    | with their eval-  uation data, for MAPE  |\n",
      "|                  | seasonality,   Quant. Finance 13 (7)     | evaluation  ELM DE provides  rock bottom |\n",
      "|                  | (2013) 1115–1123,  doi:                  | error.  For MAE, ARV, and Theils U       |\n",
      "|                  | 10.1080/14697688.2013.773458  .          | models ELM TLBO, ELM PSO and ELM Jaya    |\n",
      "|                  | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 8\u001b[0m the                    | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 8\u001b[0m the                    |\n",
      "|                  | future predictions  for 10 minutes  and  | future predictions  for 10 minutes  and  |\n",
      "|                  | 30 minutes  are done and the  system’s   | 30 minutes  are done and the  system’s   |\n",
      "|                  | performance  is measured.   4.4. Model   | performance  is measured.   4.4. Model   |\n",
      "|                  | validation   Validation  is an important | validation   Validation  is an important |\n",
      "|                  | step that is used to check the           | step that is used to check the           |\n",
      "|                  | performance   of the system by comparing | performance   of the system by comparing |\n",
      "|                  | the actual data with predicted  data.    | the actual data with predicted  data.    |\n",
      "|                  | Here we have used MSE (Mean Squared      | Here we have used MSE (Mean Squared      |\n",
      "|                  | Error), RMSE (Root Mean Square Error),   | Error), RMSE (Root Mean Square Error),   |\n",
      "|                  | MAE (Mean Absolute  Error), and          | MAE (Mean Absolute  Error), and          |\n",
      "|                  | R-squared  ( 𝑅 2 ) value for measuring   | R-squared  ( 𝑅 2 ) value for measuring   |\n",
      "|                  | the  performance  of our system.  Among  | the  performance  of our system.  Among  |\n",
      "|                  | them in MSE and RMSE, the error          | them in MSE and RMSE, the error          |\n",
      "|                  | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 2\u001b[0m M.S.                   | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 10\u001b[0m                       |\n",
      "|                  | Islam and E. Hossain Soft Computing      | model, these values are 0.00084,         |\n",
      "|                  | Letters 3 (2021) 100009  Fig. 1. Simple  | 0.02895,  0.01448  and 0.93690  respec-  |\n",
      "|                  | architecture  of our proposed  pipeline. | tively. The actual vs predicted  value   |\n",
      "|                  | USDINR.  They tested their model against | curves are provided  in Figs. 7 and  8 . |\n",
      "|                  | other models supported  by  ELM, NN, and | The x-axis indicates  the number  of     |\n",
      "|                  | FLANN.  They found that between  ELM,    | test samples  (14871  for 10-mins        |\n",
      "|                  | NN, and FLANN,   ELM shows the most      | model and 37224 samples  for 30-mins     |\n",
      "|                  | eﬀective  optimization.  Consistent      | model) that have been used for           |\n",
      "|                  | with their eval-  uation data, for MAPE  | prediction.  The actual closing values   |\n",
      "|                  | evaluation  ELM DE provides  rock bottom | of the currency  pairs are marked   by   |\n",
      "|                  | error.  For MAE, ARV, and Theils U       | yellow color and model predicted         |\n",
      "|                  | models ELM TLBO, ELM PSO and ELM Jaya    | closing values are marked  by blue       |\n",
      "|                  | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 10\u001b[0m                       |                                          |\n",
      "|                  | model, these values are 0.00084,         |                                          |\n",
      "|                  | 0.02895,  0.01448  and 0.93690  respec-  |                                          |\n",
      "|                  | tively. The actual vs predicted  value   |                                          |\n",
      "|                  | curves are provided  in Figs. 7 and  8 . |                                          |\n",
      "|                  | The x-axis indicates  the number  of     |                                          |\n",
      "|                  | test samples  (14871  for 10-mins        |                                          |\n",
      "|                  | model and 37224 samples  for 30-mins     |                                          |\n",
      "|                  | model) that have been used for           |                                          |\n",
      "|                  | prediction.  The actual closing values   |                                          |\n",
      "|                  | of the currency  pairs are marked   by   |                                          |\n",
      "|                  | yellow color and model predicted         |                                          |\n",
      "|                  | closing values are marked  by blue       |                                          |\n",
      "|                  | \u001b[31mfile1.pdf\u001b[0m \u001b[31mpage: 9\u001b[0m                        |                                          |\n",
      "|                  | 5.1.1. EUR/USD   For the EUR/USD         |                                          |\n",
      "|                  | currency  pair, we validated  the model  |                                          |\n",
      "|                  | against   14886 samples  for our 10-mins |                                          |\n",
      "|                  | model and 3723 samples  for our 30-mins  |                                          |\n",
      "|                  | model that is 20% of our total data      |                                          |\n",
      "|                  | respectively.  The model is trained      |                                          |\n",
      "|                  | using the rest of the data. Figs. 3 and  |                                          |\n",
      "|                  | 4 present  the distribution  of dif-     |                                          |\n",
      "|                  | ferences  between  actual and predicted  |                                          |\n",
      "|                  | curve provided  in Figs. 5 and  6 ,      |                                          |\n",
      "|                  | respectively.  The x -axis represents    |                                          |\n",
      "|                  | the diﬀerence  between  the actual       |                                          |\n",
      "+------------------+------------------------------------------+------------------------------------------+\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Wraptext function for prettytable\n",
    "def wrap_text(text, width=40):\n",
    "    return \"\\n\".join(textwrap.wrap(text, width=width))\n",
    "\n",
    "# Create a table with predefined row names\n",
    "row_names = [\"combined_results\", \"condensed_result\", \"combined_sources\"]\n",
    "\n",
    "# After processing all files, create and print the tables\n",
    "for file_name, queries_results in output_table.items():\n",
    "    # Create a table with 'Type' as the leftmost column\n",
    "    table_data = [[\"Type\"] + queries]\n",
    "    \n",
    "    # Add rows to the table\n",
    "    for result_type in row_names:\n",
    "        row = [result_type.capitalize()]\n",
    "        for query in queries:\n",
    "            if result_type == \"combined_sources\":\n",
    "                sources_str = \"\\n\".join([f\"\\n{colored(k[0], 'red')} {colored(k[1], 'red')}\\n{v}\" for k, v in queries_results[query][result_type].items()])\n",
    "                row.append(wrap_text(sources_str))\n",
    "            else:\n",
    "                row.append(wrap_text(queries_results[query][result_type]))\n",
    "        table_data.append(row)\n",
    "\n",
    "    # Print the table for the current file_name with a separator between rows\n",
    "    print(f\"File: {file_name}\\n{tabulate(table_data, headers='firstrow', tablefmt='grid')}\\n{'=' * 80}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```qa_all_at_once_iterated```\n",
    "Searches for the answer through all documents. Can also take the chat history into consideration. Does iterated prompts to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_all_at_once(folder_path, chain_type, chunk_size, query, k, own_knowledge = False, show_pages=False):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Read-in and split the documents\n",
    "    \"\"\"\n",
    "    all_pages = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if file_name.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_name.endswith('.csv'):\n",
    "            loader = CSVLoader(file_path)\n",
    "        elif file_name.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif file_name.endswith('.md'):\n",
    "            loader = UnstructuredMarkdownLoader(file_path)\n",
    "        else:\n",
    "            continue  # Skip files with other extensions\n",
    "\n",
    "        file = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_size/2)\n",
    "        pages = text_splitter.split_documents(file)\n",
    "        all_pages.extend(pages)\n",
    "\n",
    "    if show_pages:\n",
    "        print(len(all_pages))\n",
    "        for page in all_pages:\n",
    "            print(page)\n",
    "\n",
    "    \"\"\"\n",
    "    Vectorstores\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    db = FAISS.from_documents(all_pages, embeddings)\n",
    "    # FAISS vectorstores can also be merged and saved to disk\n",
    "\n",
    "    \"\"\"\n",
    "    Retriever\n",
    "    \"\"\"\n",
    "    # Amount of returned documents k\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "\n",
    "    \"\"\"\n",
    "    Chains\n",
    "    \"\"\"\n",
    "\n",
    "    # Define Chain\n",
    "    if own_knowledge:\n",
    "        prompt_template = \"\"\"Use the following pieces of chat history and context to answer the question at the end. \\\n",
    "            If the answer does not become clear from the context, you can also use your own knowledge. \\\n",
    "            If you use your own knowledge, please indicate this clearly in your answer. \\\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        {question}\n",
    "        Helpful answer:\"\"\"\n",
    "\n",
    "    if not own_knowledge:\n",
    "\n",
    "        prompt_template = \"\"\"Use the following pieces of chat history and context to answer the question at the end. \\\n",
    "            Do NOT use your own knowledge and give the best possible answer from the context.\\\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        {question}\n",
    "        Helpful answer:\"\"\"\n",
    "\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\",\"question\"]\n",
    "    )\n",
    "\n",
    "    chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "    # Define Chain\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=OpenAI(temperature=0),\n",
    "        chain_type=chain_type,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs=chain_type_kwargs\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Run Chain with parameters\n",
    "    result = qa(query)\n",
    "\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the chat history: \n",
      " {} \n",
      "And this is the current question: \n",
      " What is a LSTM model?.\n"
     ]
    }
   ],
   "source": [
    "# Define the Query\n",
    "query = \"What is a LSTM model?\"\n",
    "\n",
    "# Update the query with the Chat History\n",
    "query_with_context = f\"This is the chat history: \\n {str(chat_history)} \\nAnd this is the current question: \\n {query}.\"\n",
    "print(query_with_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results\n",
    "In case you do not want the chat history to be part of the prompt, change ```query=query```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[32mAnswer:\u001b[0m\n",
      "--------------------------------------------------------------------\n",
      "\n",
      " A Long Short-Term Memory (LSTM) model is a type of Recurrent Neural Network\n",
      "(RNN) that is capable of learning long-term dependencies. It is composed of four\n",
      "layers: an input layer, a memory unit, a cell state, and an output layer. The\n",
      "key component of the LSTM is the cell state, which runs straight down the entire\n",
      "timesteps with only minor but important interactions. LSTM can add or remove\n",
      "information from the cell state using several gates, each of which is made of a\n",
      "sigmoid neural network layer. These sigmoid layers produce output numbers\n",
      "between 0 and 1, which represent how much information is kept or removed from\n",
      "the cell state. LSTM models can be trained using an optimization algorithm like\n",
      "gradient descent on a set of training sequences.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[32mSources:\u001b[0m\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "('file1.pdf', 'page: 5')\n",
      "('file2.pdf', 'page: 5')\n",
      "('file1.pdf', 'page: 5')\n",
      "('file2.pdf', 'page: 5')\n"
     ]
    }
   ],
   "source": [
    "# Get Results\n",
    "result = qa_all_at_once(folder_path=folder_path, \n",
    "            chain_type=\"stuff\",\n",
    "            chunk_size=1000, \n",
    "            query=query_with_context, \n",
    "            k=4, \n",
    "            own_knowledge = True, \n",
    "            show_pages=False)\n",
    "\n",
    "# Append Queries and Answers to Chat History\n",
    "chat_history[query] = result['result']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Define Sources\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sources \u001b[39m=\u001b[39m [(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(doc\u001b[39m.\u001b[39mmetadata[\u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m]), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpage: \u001b[39m\u001b[39m{\u001b[39;00mdoc\u001b[39m.\u001b[39mmetadata[\u001b[39m'\u001b[39m\u001b[39mpage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      4\u001b[0m \u001b[39m# Sort sources by filename and page number\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sorted_sources \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(sources, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: (x[\u001b[39m0\u001b[39m], \u001b[39mint\u001b[39m(x[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m])))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# Define Sources\n",
    "sources = [(os.path.basename(doc.metadata[\"source\"]), f\"page: {doc.metadata['page']}\") for doc in result['source_documents']]\n",
    "\n",
    "# Sort sources by filename and page number\n",
    "sorted_sources = sorted(sources, key=lambda x: (x[0], int(x[1].split(\" \")[1])))\n",
    "\n",
    "# Print Answer and Sources\n",
    "print(\"\\n--------------------------------------------------------------------\")\n",
    "print(colored(\"Answer:\", \"green\"))\n",
    "print(\"--------------------------------------------------------------------\\n\")\n",
    "print(textwrap.fill(result['result'], width=80))\n",
    "print(\"\\n--------------------------------------------------------------------\")\n",
    "print(colored(\"Sources:\", \"green\"))\n",
    "print(\"--------------------------------------------------------------------\\n\")\n",
    "for source in sorted_sources:\n",
    "    print(source)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```qa_all_at_once_iterated```\n",
    "Searches for the answer through all documents. Can also take the chat history into consideration. Does iterated prompts to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_all_at_once_iterated(folder_path, chain_type, chunk_size, query, k, num_iterations, own_knowledge = False, show_pages=False):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Read-in and split the documents\n",
    "    \"\"\"\n",
    "    all_pages = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if file_name.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_name.endswith('.csv'):\n",
    "            loader = CSVLoader(file_path)\n",
    "        elif file_name.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif file_name.endswith('.md'):\n",
    "            loader = UnstructuredMarkdownLoader(file_path)\n",
    "        else:\n",
    "            continue  # Skip files with other extensions\n",
    "\n",
    "        file = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_size/2)\n",
    "        pages = text_splitter.split_documents(file)\n",
    "        all_pages.extend(pages)\n",
    "\n",
    "    if show_pages:\n",
    "        print(len(all_pages))\n",
    "        for page in all_pages:\n",
    "            print(page)\n",
    "\n",
    "    \"\"\"\n",
    "    Vectorstores\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    db = FAISS.from_documents(all_pages, embeddings)\n",
    "    # FAISS vectorstores can also be merged and saved to disk\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Prompts\n",
    "    \"\"\"\n",
    "\n",
    "    # Define Chain\n",
    "    if own_knowledge:\n",
    "        prompt_template = \"\"\"Use the following pieces of chat history and context to answer the question at the end. \\\n",
    "            If the answer does not become clear from the context, you can also use your own knowledge. \\\n",
    "            If you use your own knowledge, please indicate this clearly in your answer. \\\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        {question}\n",
    "        Helpful answer:\"\"\"\n",
    "\n",
    "    if not own_knowledge:\n",
    "\n",
    "        prompt_template = \"\"\"Use the following pieces of chat history and context to answer the question at the end. \\\n",
    "            Do NOT use your own knowledge and give the best possible answer from the context.\\\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        {question}\n",
    "        Helpful answer:\"\"\"\n",
    "\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\",\"question\"]\n",
    "    )\n",
    "\n",
    "    chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Chains\n",
    "    \"\"\"\n",
    "    # Define summary chain\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    qa_condense = load_summarize_chain(llm=OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "    \n",
    "    extended_answers = []\n",
    "    unique_sources = set()\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # QA chain that is adaptable\n",
    "        # Amount of returned documents k-i -> makes it adaptable. Otherwise, it would always return k documents and the output would be the same.\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": k-i})\n",
    "\n",
    "        # Define retrieval chain\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=OpenAI(temperature=0),\n",
    "            chain_type=chain_type,\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs=chain_type_kwargs\n",
    "        )\n",
    "\n",
    "        # Run Chain with parameters\n",
    "        result = qa(query)\n",
    "\n",
    "        # Get Sources\n",
    "        sources = [(doc.page_content, os.path.basename(doc.metadata[\"source\"]), f\"page: {doc.metadata['page']}\") for doc in\n",
    "                result['source_documents']]\n",
    "\n",
    "        # Append result to extended_answers\n",
    "        extended_answers.append(result['result'])\n",
    "\n",
    "        # Add sources to the unique_sources set\n",
    "        unique_sources.update(sources)\n",
    "\n",
    "\n",
    "    # Combine extended_answers\n",
    "    combined_result = ' '.join(extended_answers)\n",
    "\n",
    "    # Run the qa function on the combined_result (summary)\n",
    "    texts = text_splitter.split_text(combined_result)\n",
    "    docs = [Document(page_content=t) for t in texts[:3]]\n",
    "    \n",
    "    condensed_result = str(qa_condense.run(docs))\n",
    "\n",
    "    # Combine unique_sources\n",
    "    combined_sources = {tuple([source[1], source[2]]): source[0] for source in unique_sources}\n",
    "\n",
    "    \n",
    "    return combined_result, condensed_result, combined_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the chat history: \n",
      " {} \n",
      "And this is the current question: \n",
      " What is a LSTM model?.\n"
     ]
    }
   ],
   "source": [
    "# Define the Query\n",
    "query = \"What is a LSTM model?\"\n",
    "\n",
    "# Update the query with the Chat History\n",
    "query_with_context = f\"This is the chat history: \\n {str(chat_history)} \\nAnd this is the current question: \\n {query}.\"\n",
    "print(query_with_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results\n",
    "In case you do not want the chat history to be part of the prompt, change ```query=query```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[32mCombined Answer:\u001b[0m\n",
      "--------------------------------------------------------------------\n",
      "\n",
      " A LSTM (Long Short Term Memory) model is a variation of a recurrent neural\n",
      "network which can be trained using an optimization algorithm like gradient\n",
      "descent on a set of training sequences. It was first introduced by Hochreiter\n",
      "and Schmidhuber in 1997 as an updated version of RNN for addressing the problems\n",
      "like vanishing gradient and later was simplified or refined. LSTM is capable of\n",
      "learning long term dependencies and is capable of remembering for a long period\n",
      "of time using a memory unit.  A LSTM (Long Short Term Memory) model is a\n",
      "variation of a recurrent neural network which can be trained using an\n",
      "optimization algorithm like gradient descent on a set of training sequences. It\n",
      "was first introduced by Hochreiter and Schmidhuber in 1997 as an updated version\n",
      "of RNN for addressing the problems like vanishing gradient and later were\n",
      "simplified or re-engineered for better performance in time series prediction.  A\n",
      "LSTM (Long Short Term Memory) model is a variation of a recurrent neural network\n",
      "which can be trained using an optimization algorithm like gradient descent on a\n",
      "set of training sequences. It was first introduced by Hochreiter and Schmidhuber\n",
      "in 1997 as an updated version of RNN for addressing the problems like vanishing\n",
      "gradient and later were simplified or re-introduced in time series prediction.\n",
      "A Long Short Term Memory (LSTM) model is a variation of a recurrent neural\n",
      "network which can be trained using an optimization algorithm like gradient\n",
      "descent on a set of training sequences. It was first introduced by Hochreiter\n",
      "and Schmidhuber in 1997 as an updated version of RNN for addressing problems\n",
      "like vanishing gradient and later was simplified or re-introduced in time series\n",
      "prediction.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[32mCondensed Answer:\u001b[0m\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "  A Long Short Term Memory (LSTM) model is a variation of a recurrent neural\n",
      "network which was first introduced by Hochreiter and Schmidhuber in 1997 to\n",
      "address problems like vanishing gradient. It can be trained using an\n",
      "optimization algorithm like gradient descent on a set of training sequences and\n",
      "is capable of learning long term dependencies and remembering for a long period\n",
      "of time. It has been refined and re-introduced in time series prediction.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[32mCombined Sources:\u001b[0m\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "('file1.pdf', 'page: 1')\n",
      "\n",
      "('file1.pdf', 'page: 9')\n",
      "\n",
      "('file1.pdf', 'page: 5')\n",
      "\n",
      "('file1.pdf', 'page: 11')\n",
      "\n",
      "('file1.pdf', 'page: 7')\n",
      "\n",
      "('file1.pdf', 'page: 0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Results\n",
    "combined_result, condensed_result, combined_sources = qa_all_at_once_iterated(folder_path=folder_path, \n",
    "            chain_type=\"stuff\",\n",
    "            chunk_size=500, \n",
    "            query=query_with_context, \n",
    "            k=8, \n",
    "            num_iterations=4,\n",
    "            own_knowledge = True, \n",
    "            show_pages=False)\n",
    "\n",
    "# Append Queries and Answers to Chat History\n",
    "chat_history[query] = condensed_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[32mCombined Answer:\u001b[0m\n",
      "--------------------------------------------------------------------\n",
      "\n",
      " A LSTM (Long Short Term Memory) model is a variation of a recurrent neural\n",
      "network which can be trained using an optimization algorithm like gradient\n",
      "descent on a set of training sequences. It was first introduced by Hochreiter\n",
      "and Schmidhuber in 1997 as an updated version of RNN for addressing the problems\n",
      "like vanishing gradient and later was simplified or refined. LSTM is capable of\n",
      "learning long term dependencies and is capable of remembering for a long period\n",
      "of time using a memory unit.  A LSTM (Long Short Term Memory) model is a\n",
      "variation of a recurrent neural network which can be trained using an\n",
      "optimization algorithm like gradient descent on a set of training sequences. It\n",
      "was first introduced by Hochreiter and Schmidhuber in 1997 as an updated version\n",
      "of RNN for addressing the problems like vanishing gradient and later were\n",
      "simplified or re-engineered for better performance in time series prediction.  A\n",
      "LSTM (Long Short Term Memory) model is a variation of a recurrent neural network\n",
      "which can be trained using an optimization algorithm like gradient descent on a\n",
      "set of training sequences. It was first introduced by Hochreiter and Schmidhuber\n",
      "in 1997 as an updated version of RNN for addressing the problems like vanishing\n",
      "gradient and later were simplified or re-introduced in time series prediction.\n",
      "A Long Short Term Memory (LSTM) model is a variation of a recurrent neural\n",
      "network which can be trained using an optimization algorithm like gradient\n",
      "descent on a set of training sequences. It was first introduced by Hochreiter\n",
      "and Schmidhuber in 1997 as an updated version of RNN for addressing problems\n",
      "like vanishing gradient and later was simplified or re-introduced in time series\n",
      "prediction.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[32mCondensed Answer:\u001b[0m\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "  A Long Short Term Memory (LSTM) model is a variation of a recurrent neural\n",
      "network which was first introduced by Hochreiter and Schmidhuber in 1997 to\n",
      "address problems like vanishing gradient. It can be trained using an\n",
      "optimization algorithm like gradient descent on a set of training sequences and\n",
      "is capable of learning long term dependencies and remembering for a long period\n",
      "of time. It has been refined and re-introduced in time series prediction.\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[32mCombined Sources:\u001b[0m\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "('file1.pdf', 'page: 0')\n",
      "nature of this unsettled  market. This paper presents  a new model that combines  two powerful  neural networks  \n",
      "used for time series prediction:  Gated Recurrent  Unit (GRU) and Long Short Term Memory  (LSTM),  for predicting  \n",
      "the future closing prices of FOREX currencies.  The ﬁrst layer of our proposed  model is the GRU layer with 20 \n",
      "hidden neurons  and the second layer is the LSTM layer with 256 hidden neurons.  We have applied our model on\n",
      "\n",
      "('file1.pdf', 'page: 1')\n",
      "The main aim of this research  is to demonstrate  the combined  power \n",
      "of two of the most powerful  time-series  analyzers:  Gated Recurrent  Unit \n",
      "(GRU) and Long Short Term Memory  (LSTM),  to predict FOREX cur- \n",
      "rency price. For this purpose,  we have developed  a hybrid model that \n",
      "has a GRU at the front layer and LSTM at the back. We applied  our \n",
      "proposed  model to predict the closing price of four major FOREX cur- \n",
      "rency pairs: EUR/USD,  GBP/USD,  USD/CAD,  and USD/CHF.  As a proof\n",
      "\n",
      "('file1.pdf', 'page: 5')\n",
      "will be close to 1, allowing  the majority  of the past information  to be \n",
      "kept. \n",
      "3.2. Long short term memory  \n",
      "An LSTM is another  variation  of recurrent  neural network  which can \n",
      "be trained  using an optimization  algorithm  like gradient  descent  on a set \n",
      "of the training  sequence.  LSTM was ﬁrst introduced  by Hochreiter  and \n",
      "Schmidhuber  [63] in 1997 as an updated  version  of RNN for addressing  \n",
      "the problems  like vanishing  gradient  and later were simpliﬁed  or re-\n",
      "\n",
      "('file1.pdf', 'page: 7')\n",
      "tains LSTM with 256 hidden neurons.  The third layer and fourth layers \n",
      "are dense layers with 64 and 1 hidden neurons  respectively.  We have \n",
      "trained  this model using the 10 minutes  and 30 minutes  interval  data \n",
      "which we have processed  from the original  1-minute  interval  data. The\n",
      "\n",
      "('file1.pdf', 'page: 9')\n",
      "individual  models and its not possible  to understand  the diﬀerence  with- \n",
      "out proper comparison.  Another  reason for choosing  these two models is \n",
      "their performance,  which is better than other deep learning  approaches  \n",
      "[16] in time series prediction.  Both of the GRU and LSTM model were \n",
      "trained  using the same data- sets, same hidden layer formation  and was \n",
      "run 100 times each as our proposed  model. Moreover,  this proposed\n",
      "\n",
      "('file1.pdf', 'page: 11')\n",
      "GRU-LSTM  model against  a standalone  GRU model, a standalone  LSTM \n",
      "model, and a simple statistical  model where we have used simple mov- \n",
      "ing average  (SMA) of previous  20-days  closing price. Moving  average  is \n",
      "used for ﬁltering  out the noise and smoothing  the price trend. We have \n",
      "considered  a 20-days  moving  average  for analyzing  the performance  as \n",
      "a 20-days  moving  average  is proven to provide  the best result [70] .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Answer and Sources\n",
    "print(\"\\n--------------------------------------------------------------------\")\n",
    "print(colored(\"Combined Answer:\", \"green\"))\n",
    "print(\"--------------------------------------------------------------------\\n\")\n",
    "print(textwrap.fill(combined_result, width=80))\n",
    "print(\"\\n--------------------------------------------------------------------\")\n",
    "print(colored(\"Condensed Answer:\", \"green\"))\n",
    "print(\"--------------------------------------------------------------------\\n\")\n",
    "print(textwrap.fill(condensed_result, width=80))\n",
    "print(\"\\n--------------------------------------------------------------------\")\n",
    "print(colored(\"Combined Sources:\", \"green\"))\n",
    "print(\"--------------------------------------------------------------------\\n\")\n",
    "\n",
    "# Print Sources and Sort them first\n",
    "for source_key, source_element in sorted(combined_sources.items(), key=lambda x: (x[0][0], int(x[0][1].split(\" \")[1]))):\n",
    "    print(f'{source_key}\\n{source_element}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
